{
  "metadata": {
    "name": "Jarvis Hive Project",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Query GS data\n- Create a hive table (`wdi_gs`) against the gs wdi_2016 data.\n- Count number of rows from the wdi_gs table"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "-- NOTE semicolon is not allowed in Zeppelin\nDROP TABLE IF EXISTS wdi_gs;\nCREATE EXTERNAL TABLE wdi_gs\n(year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\nROW FORMAT DELIMITED FIELDS TERMINATED BY \u0027,\u0027 LINES TERMINATED BY \u0027\\n\u0027\nLOCATION \u0027gs://jarvis_data_eng_julienguyen/datasets/wdi_2016\u0027\nTBLPROPERTIES (\"skip.header.line.count\"\u003d\"1\");\n\n-- show table meta data\nDESCRIBE FORMATTED wdi_gs;"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DROP TABLE IF EXISTS wdi_csv_text;\r\nCREATE EXTERNAL TABLE wdi_csv_text\r\n(year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\r\nROW FORMAT DELIMITED FIELDS TERMINATED BY \u0027,\u0027 LINES TERMINATED BY \u0027\\n\u0027\r\nLOCATION \u0027hdfs:///user/julienguyen/hive/wdi/wdi_csv_text\u0027;"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "INSERT OVERWRITE TABLE wdi_csv_text\nSELECT * FROM wdi_gs"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT count(countryName) FROM wdi_csv_text"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\n\ncd ~\nhdfs  dfs -get  hdfs:///user/julienguyen/hive/wdi/wdi_csv_text .\ncd wdi_csv_text\n\n#calculate current directory size\n\ndu -ch .\n#1.8G\ttotal\n\necho 3 | sudo tee /proc/sys/vm/drop_caches\ndate +%s \u0026\u0026 cat * | wc \u0026\u0026 date +%s\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Parsing Issue\n\n- Find the parsing issue within wdi_csv_text by creating a debug table\n- Create a table using OpenCSV Serde"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT distinct(indicatorcode)\r\nFROM wdi_csv_text\r\nORDER BY indicatorcode\r\nLIMIT 20;"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DROP TABLE IF EXISTS wdi_gs_debug;\r\nCREATE EXTERNAL TABLE wdi_gs_debug\r\n(line STRING) \r\nLOCATION \u0027hdfs:///user/julienguyen/hive/wdi/wdi_csv_text\u0027\r\nTBLPROPERTIES (\"skip.header.line.count\"\u003d\"1\")"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT line FROM wdi_gs_debug\nWHERE line like \"%\\(\\% of urban population\\)\\\"%\""
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Parsing Issue\n\nThe CSV parsing showed issues with incomplete column names, containing columns that did not exist. In order to debug the problem, it is efficient to output the whole row to see where the parsing issue arises. The CSV parsing failed because the SerDe parser splits columns by looking for a comma. The IndicatorCode attribute contains a comma between the code (ex. \"Population living in slums, (% of urban population)\") which causes the SerDe parser to split one column into two different ones. In order to fix this issue, it would be done by being more specific with specific cases when parsing data."
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DROP TABLE IF EXISTS wdi_opencsv_gs;\r\nCREATE EXTERNAL TABLE wdi_opencsv_gs\r\n(year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\r\nROW FORMAT SERDE \u0027org.apache.hadoop.hive.serde2.OpenCSVSerde\u0027\r\nLOCATION \u0027gs://jarvis_data_eng_julienguyen/datasets/wdi_2016\u0027"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DROP TABLE IF EXISTS wdi_opencsv_text;\nCREATE EXTERNAL TABLE wdi_opencsv_text\n(year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\nROW FORMAT SERDE \u0027org.apache.hadoop.hive.serde2.OpenCSVSerde\u0027\nLOCATION \u0027hdfs:///user/julienguyen/hive/wdi/wdi_opencsv_text\u0027"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "INSERT OVERWRITE TABLE wdi_opencsv_text\nSELECT * FROM wdi_opencsv_gs"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT distinct(indicatorcode)\r\nFROM wdi_opencsv_text\r\nORDER BY indicatorcode\r\nLIMIT 20;"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT count(countryName) FROM wdi_opencsv_text"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT count(countryName) FROM wdi_csv_text"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Compare execution time between `wdi_opencsv_text` and `wdi_csv_text`\n\nWhen comparing `wdi_opencsv_text` and `wdi_csv_text` execution times, it is observed that `wdi_opencsv_text` is significantly slower to compile. The reason for this is that Serde parsing is slow to compile as it generates significant amounts of generic code. It is typically complex and costly to run Serde for serialization/deserialization. Additionally, the parser splits columns more accurately by considering more specific cases, such as commas within quotation marks not being counted. This limits the performance in order to provide more accuracy."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# OpenCSVSerde Limitation\n\n- Create an OpenCSV Serde view and cast the column data_type to be correct."
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DESCRIBE FORMATTED wdi_opencsv_text"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DESCRIBE FORMATTED wdi_csv_text"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Compare `wdi_opencsv_text` and `wdi_csv_text` metadata\n\nThe data type is deserialized to all strings in `wdi_opencsv_text`, despite that some columns such as `year` and `indicatorvalue` should be int or float data types. Meanwhile, the `wdi_csv_text` converts all columns to their expected data type. The data can be converted to their respective types for `wdi_opencsv_text` by creating a View over the table that does CAST to the desired type."
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DROP VIEW IF EXISTS wdi_opencsv_text_view;\r\n\r\nCREATE VIEW IF NOT EXISTS wdi_opencsv_text_view\r\nAS\r\nSELECT CAST(year AS INTEGER), countryName, countryCode, indicatorName, indicatorCode, CAST(indicatorValue AS FLOAT) FROM wdi_opencsv_text"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DESCRIBE FORMATTED wdi_opencsv_text_view"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Write a HiveQL to find the 2015 GDP growth (annual %) for Canada\n\n1. Write an HQL to find out the correct indicator name/code.\n2. Write an HQL to find the 2015 Canada GDP growth."
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DROP VIEW IF EXISTS wdi_opencsv_gdp;\n\nCREATE VIEW IF NOT EXISTS wdi_opencsv_gdp\nAS\n    SELECT CAST(indicatorvalue AS FLOAT) AS GDP_growth_value, CAST(year AS INTEGER), countryname\n    FROM wdi_opencsv_text\n    WHERE countryname \u003d \u0027Canada\u0027 and indicatorname LIKE (\u0027%GDP growth (annual \\%)%\u0027) and year \u003d \u00272015\u0027\n"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\nSELECT * FROM wdi_opencsv_gdp"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 2015 Canada GDP Growth HQL\n\nThe above query is inefficient and takes a long time to compile because there\u0027s too much data. The query is reading from the entire table of over a million records in order to find a single one. In order to optimize the query, it may be more efficient to partition the tables into groupings such as records from 2015. This will allow for a smaller dataset to be queried, as well as allow us to reuse the table for future queries."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Hive Partitions\n\n- Optimize the previous query using Hive Partitions"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DROP TABLE IF EXISTS wdi_opencsv_text_partitions;\nCREATE EXTERNAL TABLE wdi_opencsv_text_partitions \n(countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT) PARTITIONED BY (year INTEGER)\nROW FORMAT SERDE \u0027org.apache.hadoop.hive.serde2.OpenCSVSerde\u0027\nLOCATION \u0027hdfs:///user/julienguyen/hive/wdi/wdi_opencsv_text_partition\u0027"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "set hive.exec.dynamic.partition.mode\u003dnonstrict;\nset hive.exec.dynamic.partition\u003dtrue;\nset hive.stats.column.autogather\u003dfalse;\nFROM wdi_opencsv_text\nINSERT OVERWRITE TABLE wdi_opencsv_text_partitions PARTITION (year)\n    SELECT countryName, countryCode, indicatorName, indicatorCode, indicatorValue, year"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\n\nhdfs dfs -du -h hdfs:///user/julienguyen/hive/wdi/wdi_opencsv_text_partition\n#hdfs dfs -du -h hdfs:///user/julienguyen/hive/wdi/wdi_opencsv_text"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DROP VIEW IF EXISTS wdi_opencsv_gdp_partitions;\n\nCREATE VIEW IF NOT EXISTS wdi_opencsv_gdp_partitions\nAS\n    SELECT CAST(indicatorvalue AS FLOAT) AS GDP_growth_value, CAST(year AS INTEGER), countryname\n    FROM wdi_opencsv_text_partitions\n    WHERE countryname \u003d \u0027Canada\u0027 and indicatorname LIKE (\u0027%GDP growth (annual \\%)%\u0027) and year \u003d \u00272015\u0027\n"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT * FROM wdi_opencsv_gdp_partitions"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 2015 GDP Growth With Hive Partitions\n\nThe time taken to execute the 2015 Canada GDP Growth HQL has significantly decreased when using `wdi_opencsv_text_partitions` instead of `wdi_opencsv_text`. This was because there is less data in the tables, therefore it was only required to query data where `year\u003d2015`, cutting the amount of data by at least 50x as there are more than 50 years listed in the dataset."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Columnar File Optimization\n\n- Create a table that is stored as `parquet`\n- Load data from `wdi_opencsv_gs` to `wdi_csv_parquet`\n- Compare runtime by executing a query that counts the number of records in each table."
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DROP TABLE IF EXISTS wdi_csv_parquet;\nCREATE EXTERNAL TABLE wdi_csv_parquet\n(year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\nSTORED AS PARQUET\nLOCATION \u0027hdfs:///user/julienguyen/hive/wdi/wdi_csv_parquet\u0027"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "set hive.exec.dynamic.partition.mode\u003dnonstrict;\nINSERT OVERWRITE TABLE wdi_csv_parquet\n    SELECT year, countryName, countryCode, indicatorName, indicatorCode, indicatorValue FROM wdi_opencsv_gs"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\n\nhdfs dfs -du hdfs:///user/julienguyen/hive/wdi/wdi_csv_parquet\nhdfs dfs -du hdfs:///user/julienguyen/hive/wdi/wdi_opencsv_text"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Compare file sizes betwen `wdi_csv_parquet` and `wdi_opencsv_text`\n\n\nThe file size of `wdi_opencsv_text` is significantly larger than the file size of `wdi_csv_parquet`, with 1066622664 and 62772966 respectively for the first part of the dataset. This is almost 16 times greater than when stored with parquet. Using parquet optimizes the file through file compression, making it smaller with several encoding methods such as dictionary encoding, bit packing and run-length encoding (RLE)."
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT count(countryName) FROM wdi_csv_parquet;"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT count(countryName) FROM wdi_opencsv_text;"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DROP VIEW IF EXISTS wdi_csv_parquet_gdp;\n\nCREATE VIEW IF NOT EXISTS wdi_csv_parquet_gdp\nAS\n    SELECT CAST(indicatorvalue AS FLOAT) AS GDP_growth_value, CAST(year AS INTEGER), countryname\n    FROM wdi_csv_parquet\n    WHERE countryname \u003d \u0027Canada\u0027 and indicatorname LIKE (\u0027%GDP growth (annual \\%)%\u0027) and year \u003d \u00272015\u0027;"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\nSELECT * FROM wdi_csv_parquet_gdp"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT * FROM wdi_opencsv_gdp"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Compare GDP growth performance between `wdi_openscv_text` and `wdi_csv_parquet`\n\nThe dataset when queried with `wdi_csv_parquet` is significantly faster than `wdi_opencsv_text`. The parquet-based filesystem allows for the query to focus on relevant data faster. The amount of data scanned will also be much smaller and allow for less I/O usage."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Highest GDP Growth\n\n- Find the year with the highest GDP growth using the `NY.GDP.MKTP.KD.ZG` indicator code for each country.\n- Use Spark SQL to compare with Hive"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DROP VIEW IF EXISTS wdi_csv_parquet_highest_gdp;\n\nCREATE VIEW IF NOT EXISTS wdi_csv_parquet_highest_gdp\nAS\n    SELECT MAX(CAST(indicatorvalue AS FLOAT)) AS GDP_growth_value, countryName\n    FROM wdi_csv_parquet\n    WHERE indicatorCode LIKE \u0027%NY.GDP.MKTP.KD.ZG%\u0027 and indicatorvalue \u003e 0 --and (countryName LIKE \u0027Sierra Leone\u0027 OR countryName LIKE \u0027Kyrgyz Republic\u0027 OR countryName LIKE \u0027Estonia\u0027 OR countryName LIKE \u0027Belize\u0027 OR countryName LIKE \u0027Iraq\u0027)\n    GROUP BY countryname\n    ORDER BY countryname"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT DISTINCT w1.GDP_growth_value, w1.countryName, w2.year FROM wdi_csv_parquet_highest_gdp w1 JOIN wdi_csv_parquet w2 ON w1.GDP_growth_value \u003d w2.indicatorvalue and w1.countryname \u003d w2.countryname;"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\nshow tables;"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\nSELECT DISTINCT w1.GDP_growth_value, w1.countryName, w2.year FROM wdi_csv_parquet_highest_gdp w1 JOIN wdi_csv_parquet w2 ON w1.GDP_growth_value \u003d w2.indicatorvalue and w1.countryname \u003d w2.countryname"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Compare Highest GDP Growth execution time between Spark and Hive interpreter\n\nThe Hive interpreter is significantly slower when running the Highest GDP Growth query, taking about a minute while Spark only took about 20 seconds. Spark has much faster query execution than Hive.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Sort GDP by country and year\n\n- Write a query that returns the GDP growth for all countries. Sort by countryName and year."
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DROP VIEW IF EXISTS wdi_csv_parquet_gdp_all;\n\nCREATE VIEW IF NOT EXISTS wdi_csv_parquet_gdp_all\nAS\n    SELECT countryName, CAST(year AS INTEGER), indicatorCode, CAST(indicatorvalue AS FLOAT) AS GDP_growth_value\n    FROM wdi_csv_parquet\n    WHERE indicatorCode LIKE \u0027%NY.GDP.MKTP.KD.ZG%\u0027 --and (countryName LIKE \u0027South Asia\u0027)\n    GROUP BY countryname, year, indicatorCode, indicatorValue\n    ORDER BY countryname, year, indicatorCode, indicatorValue"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT * FROM wdi_csv_parquet_gdp_all"
    }
  ]
}